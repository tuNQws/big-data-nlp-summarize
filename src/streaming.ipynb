{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42246e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_HOME'] = r\"C:\\hadoop\"\n",
    "os.environ['PATH'] += r\";C:\\hadoop\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "510adbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"C:\\spark-3.5.5-bin-hadoop3\")\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizerModel\n",
    "from pyspark.ml.clustering import LDAModel\n",
    "\n",
    "scala_version = \"2.12\"\n",
    "spark_version = \"3.5.5\"\n",
    "packages = [\n",
    "    f\"org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}\",\n",
    "    \"org.apache.kafka:kafka-clients:3.6.0\"\n",
    "]\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .appName(\"kafka-example\") \\ \n",
    "#     .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "#     .getOrCreate()\n",
    "    \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedditTopicModeling\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .config(\"spark.hadoop.io.nativeio.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"600\") \\\n",
    "    .getOrCreate()\n",
    "    # .config(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\") \\\n",
    "    # .config(\"spark.hadoop.fs.AbstractFileSystem.file.impl\", \"org.apache.hadoop.fs.local.LocalFs\") \\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eca3c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, concat_ws, from_unixtime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType\n",
    "import os\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add(\"id\", StringType()) \\\n",
    "    .add(\"title\", StringType()) \\\n",
    "    .add(\"selftext\", StringType()) \\\n",
    "    .add(\"created_utc\", LongType())\n",
    "\n",
    "raw = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"reddit_posts\") \\\n",
    "    .load()\n",
    "\n",
    "posts = (raw\n",
    "    .selectExpr(\"CAST(value AS STRING) AS json\")\n",
    "    .select(from_json(col(\"json\"), schema).alias(\"data\"))\n",
    "    .select(\"data.id\", \"data.title\", \"data.selftext\", \"data.created_utc\")\n",
    "    \n",
    "    # ← here we add your `text` column and a proper timestamp\n",
    "    .withColumn(\"text\", concat_ws(\" \", col(\"title\"), col(\"selftext\")))\n",
    "    .withColumn(\"timestamp\",\n",
    "                from_unixtime(col(\"created_utc\")).cast(TimestampType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "190d8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic.load(\"modeling/bertopic\")\n",
    "info = topic_model.get_topic_info()  \n",
    "label_map = dict(zip(info[\"Topic\"], info[\"Name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b63a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are streaming!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.streaming import StreamingQuery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import (\n",
    "    from_json, col, concat_ws, from_unixtime, pandas_udf\n",
    ")\n",
    "from pyspark.sql.functions import split, regexp_replace, col\n",
    "\n",
    "csv_path = \"output/all_results.csv\"\n",
    "checkpoint_path = \"output/checkpoints/bertopic_fbb\"\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def predict_label(texts: pd.Series) -> pd.Series:\n",
    "    model = topic_model\n",
    "    topics, _ = model.transform(texts.tolist())\n",
    "    # Với mỗi t: nếu None giữ None, else lấy label_map[t]\n",
    "    return pd.Series([\n",
    "        label_map.get(int(t), None) if t is not None else None\n",
    "        for t in topics\n",
    "    ])\n",
    "\n",
    "annotated = (posts\n",
    "    .withColumn(\"topic\", predict_label(col(\"text\")))\n",
    ")\n",
    "\n",
    "def foreach_batch(df, epoch_id):\n",
    "    # select only the columns you want\n",
    "    pdf = df.select(\"text\", \"topic\") \\\n",
    "            .toPandas()\n",
    "    if pdf.empty:\n",
    "        return\n",
    "\n",
    "    # strip off the leading ID and clean underscores\n",
    "    pdf[\"topic\"] = (\n",
    "        pdf[\"topic\"]\n",
    "           .str.split(\"_\", n=1).str.get(1)     # drop the \"1_\"\n",
    "           .str.replace(\"_\", \" \", regex=False)  # underscores → spaces\n",
    "    )\n",
    "    \n",
    "    # append to a single CSV; write header only on the first batch\n",
    "    pdf.to_csv(\n",
    "        csv_path,\n",
    "        mode=\"a\",\n",
    "        index=False,\n",
    "        header=(epoch_id == 0)\n",
    "    )\n",
    "\n",
    "if posts.isStreaming:\n",
    "    print(\"We are streaming!\")\n",
    "    # (posts.writeStream\n",
    "    #     .foreachBatch(foreach_batch)\n",
    "    #     .option(\"checkpointLocation\", \"./tmp/checkpoints/bertopic\")\n",
    "    #     .trigger(processingTime=\"5 seconds\")\n",
    "    #     .start()\n",
    "    #     .awaitTermination())\n",
    "    \n",
    "    # query = (\n",
    "    #     annotated.writeStream\n",
    "    #     # DataStream queries need to be named\n",
    "    #     .queryName(\"posts\")\n",
    "    #     .format(\"memory\")\n",
    "    #     .outputMode(\"append\")\n",
    "    #     .trigger(processingTime=\"5 seconds\")\n",
    "    #     .option(\"checkpointLocation\", r\"C:\\tmp\\spark-checkpoint\\posts_append_v23\")\n",
    "    #     .start()\n",
    "    # )\n",
    "    \n",
    "    # import time\n",
    "    # while query.isActive:\n",
    "    #     # 2) Clear the terminal window\n",
    "    #     os.system('clear')\n",
    "    #     time.sleep(5)  # phải dài hơn trigger interval\n",
    "    #     print(\"=== Latest batch snapshot ===\")\n",
    "    #     spark.table(\"posts\").select(\"text\", \"topic\").show(truncate=False)\n",
    "    \n",
    "    cleaned = annotated \\\n",
    "    .select(\n",
    "        col(\"text\"),\n",
    "        col(\"topic\")\n",
    "    )\n",
    "    \n",
    "    query = (\n",
    "        cleaned.writeStream               # file sink only supports append\n",
    "        .foreachBatch(foreach_batch)        # where to write the CSV files\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .option(\"header\", True)              # emit headers in each part file\n",
    "        .trigger(processingTime=\"5 seconds\")\n",
    "        .start()\n",
    "    )\n",
    "    \n",
    "    # Chờ query hoàn thành\n",
    "    query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
