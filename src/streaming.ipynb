{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42246e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_HOME'] = r\"C:\\hadoop\"\n",
    "os.environ['PATH'] += r\";C:\\hadoop\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510adbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop version: 3.3.4\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"C:\\spark-3.5.5-bin-hadoop3\")\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "scala_version = \"2.12\"\n",
    "spark_version = \"3.5.5\"\n",
    "packages = [\n",
    "    f\"org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}\",\n",
    "    \"org.apache.kafka:kafka-clients:3.6.0\"\n",
    "]\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .appName(\"kafka-example\") \\ \n",
    "#     .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "#     .getOrCreate()\n",
    "    \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedditTopicModeling\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .config(\"spark.hadoop.io.nativeio.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"600\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "hadoop_version = spark._jvm.org.apache.hadoop.util.VersionInfo.getVersion()\n",
    "print(\"Hadoop version:\", hadoop_version)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf4bee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\ml\\linalg\\__init__.py:81: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
      "  import scipy.sparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents ready for vectorization: 12063\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                             |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(5000,[26,346,639,649,1774,3251],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                                          |\n",
      "|(5000,[23,38,78,338,1627,1746,1862,2078,2214,2486],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                        |\n",
      "|(5000,[3,23,50,72,168,234,373,804,927,1163,1183,1586,1733,2067,2907,3588,4326],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])|\n",
      "|(5000,[23,287,315,341,1339,1348,1812,2653,2657,3326],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                      |\n",
      "|(5000,[68,100,121,257,395,503,725,770,900,2489,2514,3108,3566,3701],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                       |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, size\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"LDAExample\").getOrCreate()\n",
    "\n",
    "# 1. Đọc CSV\n",
    "df = spark.read.csv(\"modeling/data/reddit_posts_clean.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2. Lọc bỏ dòng chưa có clean_text và ép kiểu\n",
    "df = (\n",
    "    df.filter(col(\"clean_text\").isNotNull())\n",
    "      .withColumn(\"clean_text\", col(\"clean_text\").cast(\"string\"))\n",
    ")\n",
    "\n",
    "# 3. Tokenize\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"clean_text\", \n",
    "    outputCol=\"tokens\", \n",
    "    pattern=\"\\\\W+\", \n",
    "    minTokenLength=2\n",
    ")\n",
    "df_tok = tokenizer.transform(df)\n",
    "\n",
    "# 4. Remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "df_fil = remover.transform(df_tok)\n",
    "\n",
    "# 5. Loại bỏ documents không còn token nào\n",
    "df_ok = df_fil.filter(col(\"filtered\").isNotNull()) \\\n",
    "              .filter(size(col(\"filtered\")) > 0)\n",
    "\n",
    "print(\"Documents ready for vectorization:\", df_ok.count())\n",
    "\n",
    "# 6. Vectorize\n",
    "cv = CountVectorizer(\n",
    "    inputCol=\"filtered\",\n",
    "    outputCol=\"features\",\n",
    "    vocabSize=5000,\n",
    "    minDF=5      # hoặc giảm xuống nếu còn ít docs\n",
    ")\n",
    "model_cv = cv.fit(df_ok)\n",
    "df_featurized = model_cv.transform(df_ok)\n",
    "\n",
    "df_featurized.select(\"features\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd09ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|keywords                                        |termWeights                                                                                                        |\n",
      "+-----+------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[games, died, weeks, left, told]                |[0.00221603755765598, 0.0020612517617784125, 0.002055028273079264, 0.001655495702353431, 0.001507592896189887]     |\n",
      "|1    |[court, supreme, trump, citizenship, birthright]|[0.009282702062019522, 0.00831119293345803, 0.0060545753613106025, 0.004093753677551748, 0.0036899866467082093]    |\n",
      "|2    |[homemade, trump, anime, ate, chicken]          |[0.013647862521525467, 0.011382129513244313, 0.005096628168955756, 0.004252448487533014, 0.0037714866903176786]    |\n",
      "|3    |[soldiers, idf, june, press, pounds]            |[0.003310759143688353, 0.003170328701518064, 0.0022018077028285164, 0.002055666188704195, 0.0019607534854003622]   |\n",
      "|4    |[ng, th, nh, ch, kh]                            |[0.02004305004901656, 0.009043020905243452, 0.008391416138407035, 0.006023541491004882, 0.005097052682782689]      |\n",
      "|5    |[de, que, eu, az, se]                           |[0.010503150152734095, 0.005215356653655371, 0.0029600967278172155, 0.0025558656649410576, 0.0023139214298383247]  |\n",
      "|6    |[girl, long, republican, gabbard, stores]       |[0.002033283756643929, 0.0010243312589380122, 9.256086968230965E-4, 7.998468217747371E-4, 7.898160097271961E-4]    |\n",
      "|7    |[like, time, one, get, study]                   |[0.009908427331881813, 0.006815982938842469, 0.006229184028621719, 0.005980115492493838, 0.005593156959981008]     |\n",
      "|8    |[writing, academic, essay, ai, pick]            |[0.005859885680212296, 0.0040786752483983665, 0.003978160549110616, 0.0033592742743426916, 0.00326087714433226]    |\n",
      "|9    |[pakistan, india, indian, attack, russian]      |[0.0069064062822365505, 0.005896407416264806, 0.004606389592041757, 0.00369013732496758, 0.0033745661678862594]    |\n",
      "|10   |[study, looking, buddy, need, someone]          |[0.02188548024301089, 0.010341186769676897, 0.008291507150951248, 0.007022836100871012, 0.006734204525863846]      |\n",
      "|11   |[captain, meme, says, jd, tourist]              |[0.0014051277918054137, 0.0012893568989800713, 0.001191727692269624, 0.001143811099008023, 0.0011192630532504948]  |\n",
      "|12   |[ate, year, instead, onion, draw]               |[0.002047278977891145, 0.0020158306725917385, 0.0016360837187358738, 0.001615395073879974, 0.0014263464240315508]  |\n",
      "|13   |[club, la, players, michigan, saving]           |[0.0020519302876405615, 0.0017354077971770872, 0.0017335544308493258, 0.0016638529135630635, 0.0016201519435557093]|\n",
      "|14   |[da, rfk, jr, deep, bird]                       |[0.0018416134389532218, 0.001826326928306113, 0.0017901385004550998, 0.0015686220946398784, 0.0015162637812483057] |\n",
      "+-----+------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "lda = LDA(k=15,        # tương đương num_topics=10\n",
    "          maxIter=15,\n",
    "          featuresCol=\"features\")\n",
    "ldaModel = lda.fit(df_featurized)\n",
    "\n",
    "topics = ldaModel.describeTopics(5)  # top 5 terms\n",
    "vocab  = model_cv.vocabulary\n",
    "def termsUDF(termIndices):\n",
    "    return [vocab[i] for i in termIndices]\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "udfTerms = udf(termsUDF, ArrayType(StringType()))\n",
    "topics.select(\n",
    "    \"topic\", \n",
    "    udfTerms(\"termIndices\").alias(\"keywords\"), \n",
    "    \"termWeights\"\n",
    ").show(truncate=False)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, lda])\n",
    "pipelineModel = pipeline.fit(df)          # df_ok: DataFrame tĩnh đã clean/tokenize\n",
    "\n",
    "cv_model  = pipelineModel.stages[2]\n",
    "lda_model = pipelineModel.stages[3]\n",
    "\n",
    "vocab = cv_model.vocabulary\n",
    "topic_terms = {\n",
    "    row.topic: [vocab[i] for i in row.termIndices]\n",
    "    for row in lda_model.describeTopics(5).collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca3c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, concat_ws\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add(\"id\", StringType()) \\\n",
    "    .add(\"title\", StringType()) \\\n",
    "    .add(\"selftext\", StringType()) \\\n",
    "    .add(\"created_utc\", LongType())\n",
    "\n",
    "raw = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"reddit_posts\") \\\n",
    "    .load()\n",
    "\n",
    "posts = raw.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.id\", \"data.title\", \"data.selftext\")\n",
    "    \n",
    "posts = posts.withColumn(\"clean_text\", concat_ws(\" \", col(\"title\"), col(\"selftext\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ec019f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tuanq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 2.2 Load dictionary và LDA thuần (gensim) — hoặc bạn có thể build BOW theo vocab Spark\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpora\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dictionary\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mldamodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LdaModel\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Nếu bạn đã lưu gensim model:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m4.3.3\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m     14\u001b[39m logger = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mgensim\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger.handlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[32m     16\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIndexedCorpus\u001b[39;00m(interfaces.CorpusABC):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\interfaces.py:19\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCorpusABC\u001b[39;00m(utils.SaveLoad):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\matutils.py:1034\u001b[39m\n\u001b[32m   1029\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1.\u001b[39m - \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 & set2)) / \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlogsumexp\u001b[39m(x):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\_matutils.pyx:1\u001b[39m, in \u001b[36minit gensim._matutils\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# 2.1 Hàm tokenize + remove stopwords thuần\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "token_pattern = re.compile(r\"\\b[a-zA-Z]{2,}\\b\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # lowercase, tách token, loại stopword\n",
    "    tokens = token_pattern.findall(text.lower())\n",
    "    return [t for t in tokens if t not in stop_words]\n",
    "\n",
    "# 2.2 Load dictionary và LDA thuần (gensim) — hoặc bạn có thể build BOW theo vocab Spark\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Nếu bạn đã lưu gensim model:\n",
    "dictionary = Dictionary.load(\"modeling/reddit.dict\")\n",
    "lda = LdaModel.load(\"modeling/reddit_lda.model\")\n",
    "\n",
    "# 2.3 pandas_udf để infer topic và keywords\n",
    "@pandas_udf(\"topic int, keywords string\", functionType=\"pandas_udf\")\n",
    "def infer_topics(texts: pd.Series) -> pd.DataFrame:\n",
    "    topics, keywords = [], []\n",
    "    for doc in texts:\n",
    "        toks = preprocess_text(doc)\n",
    "        bow = dictionary.doc2bow(toks)\n",
    "        # lấy topic mạnh nhất\n",
    "        topic_probs = lda.get_document_topics(bow, minimum_probability=0.0)\n",
    "        top_t, _ = max(topic_probs, key=lambda x: x[1])\n",
    "        topics.append(int(top_t))\n",
    "        # lấy top 5 terms\n",
    "        terms = lda.show_topic(top_t, topn=5)\n",
    "        keywords.append(\", \".join([w for w,_ in terms]))\n",
    "    return pd.DataFrame({\n",
    "        \"topic\":   topics,\n",
    "        \"keywords\": keywords\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62b63a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are streaming!\n",
      "=== Latest batch snapshot ===\n",
      "+---+-----+--------+----------+------+--------+--------+-----------------+--------------+\n",
      "|id |title|selftext|clean_text|tokens|filtered|features|topicDistribution|predictedTopic|\n",
      "+---+-----+--------+----------+------+--------+--------+-----------------+--------------+\n",
      "+---+-----+--------+----------+------+--------+--------+-----------------+--------------+\n",
      "\n",
      "=== Latest batch snapshot ===\n",
      "+---+-----+--------+----------+------+--------+--------+-----------------+--------------+\n",
      "|id |title|selftext|clean_text|tokens|filtered|features|topicDistribution|predictedTopic|\n",
      "+---+-----+--------+----------+------+--------+--------+-----------------+--------------+\n",
      "+---+-----+--------+----------+------+--------+--------+-----------------+--------------+\n",
      "\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = cad01fea-b58b-4d8f-a9ba-f79003fdc22b, runId = def499ea-efbe-4fc1-ba43-21be81a43d4f] terminated with exception: Job aborted due to stage failure: Task 0 in stage 85.0 failed 1 times, most recent failure: Lost task 0.0 in stage 85.0 (TID 82) (DESKTOP-5CKD8IE executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\tuanq\\AppData\\Local\\Temp\\ipykernel_14388\\1313041508.py\", line 16, in top_topic\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py\", line 10468, in map\n    return self.apply(infer).__finalize__(self, \"map\")\n           ^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py\", line 10374, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n           ^^^^^^^^^^\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py\", line 916, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py\", line 1063, in apply_standard\n    results, res_index = self.apply_series_generator()\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py\", line 1081, in apply_series_generator\n    results[i] = self.func(v, *self.args, **self.kwargs)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py\", line 10466, in infer\n    return x._map_values(func, na_action=na_action)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py\", line 921, in _map_values\n    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py\", line 1743, in map_array\n    return lib.map_infer(values, mapper, convert=convert)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"lib.pyx\", line 2972, in pandas._libs.lib.map_infer\n  File \"C:\\Users\\tuanq\\AppData\\Local\\Temp\\ipykernel_14388\\1313041508.py\", line 16, in <lambda>\nAttributeError: 'int' object has no attribute 'toArray'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStreamingQueryException\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m     spark.table(\u001b[33m\"\u001b[39m\u001b[33mposts\u001b[39m\u001b[33m\"\u001b[39m).show(truncate=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Chờ query hoàn thành\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\streaming\\query.py:221\u001b[39m, in \u001b[36mStreamingQuery.awaitTermination\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jsq.awaitTermination(\u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m))\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark-3.5.5-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mStreamingQueryException\u001b[39m: [STREAM_FAILED] Query [id = cad01fea-b58b-4d8f-a9ba-f79003fdc22b, runId = def499ea-efbe-4fc1-ba43-21be81a43d4f] terminated with exception: Job aborted due to stage failure: Task 0 in stage 85.0 failed 1 times, most recent failure: Lost task 0.0 in stage 85.0 (TID 82) (DESKTOP-5CKD8IE executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\tuanq\\AppData\\Local\\Temp\\ipykernel_14388\\1313041508.py\", line 16, in top_topic\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py\", line 10468, in map\n    return self.apply(infer).__finalize__(self, \"map\")\n           ^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py\", line 10374, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n           ^^^^^^^^^^\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py\", line 916, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py\", line 1063, in apply_standard\n    results, res_index = self.apply_series_generator()\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py\", line 1081, in apply_series_generator\n    results[i] = self.func(v, *self.args, **self.kwargs)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py\", line 10466, in infer\n    return x._map_values(func, na_action=na_action)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py\", line 921, in _map_values\n    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py\", line 1743, in map_array\n    return lib.map_infer(values, mapper, convert=convert)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"lib.pyx\", line 2972, in pandas._libs.lib.map_infer\n  File \"C:\\Users\\tuanq\\AppData\\Local\\Temp\\ipykernel_14388\\1313041508.py\", line 16, in <lambda>\nAttributeError: 'int' object has no attribute 'toArray'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "if posts_pred.isStreaming:\n",
    "    print(\"We are streaming!\")\n",
    "    # Creating a DataSreamWriter and StreamingQuery\n",
    "    # ===\n",
    "    # Calling .writeStream on a DataFrame returns an instance of DataStreamWriter\n",
    "    query = (\n",
    "        posts_pred.writeStream\n",
    "        # DataStream queries need to be named\n",
    "        .queryName(\"posts\")\n",
    "        .format(\"memory\")\n",
    "        .outputMode(\"append\")\n",
    "        .trigger(processingTime=\"5 seconds\")\n",
    "        .option(\"checkpointLocation\", r\"C:\\tmp\\spark-checkpoint\\posts_append_v11\")\n",
    "        .start()\n",
    "    )\n",
    "    \n",
    "    import time\n",
    "    while query.isActive:\n",
    "        time.sleep(5)  # phải dài hơn trigger interval\n",
    "        print(\"=== Latest batch snapshot ===\")\n",
    "        spark.table(\"posts\").show(truncate=False)\n",
    "    \n",
    "    # Chờ query hoàn thành\n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7f25d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
