{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42246e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_HOME'] = r\"C:\\hadoop\"\n",
    "os.environ['PATH'] += r\";C:\\hadoop\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510adbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"C:\\spark-3.5.5-bin-hadoop3\")\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizerModel\n",
    "from pyspark.ml.clustering import LDAModel\n",
    "\n",
    "scala_version = \"2.12\"\n",
    "spark_version = \"3.5.5\"\n",
    "packages = [\n",
    "    f\"org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}\",\n",
    "    \"org.apache.kafka:kafka-clients:3.6.0\"\n",
    "]\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .appName(\"kafka-example\") \\ \n",
    "#     .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "#     .getOrCreate()\n",
    "    \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedditTopicModeling\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .config(\"spark.hadoop.io.nativeio.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"600\") \\\n",
    "    .getOrCreate()\n",
    "    # .config(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\") \\\n",
    "    # .config(\"spark.hadoop.fs.AbstractFileSystem.file.impl\", \"org.apache.hadoop.fs.local.LocalFs\") \\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca3c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, concat_ws, from_unixtime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType\n",
    "import os\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add(\"id\", StringType()) \\\n",
    "    .add(\"title\", StringType()) \\\n",
    "    .add(\"selftext\", StringType()) \\\n",
    "    .add(\"created_utc\", LongType())\n",
    "\n",
    "raw = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"reddit_posts\") \\\n",
    "    .load()\n",
    "\n",
    "posts = (raw\n",
    "    .selectExpr(\"CAST(value AS STRING) AS json\")\n",
    "    .select(from_json(col(\"json\"), schema).alias(\"data\"))\n",
    "    .select(\"data.id\", \"data.title\", \"data.selftext\", \"data.created_utc\")\n",
    "    \n",
    "    # ← here we add your `text` column and a proper timestamp\n",
    "    .withColumn(\"text\", concat_ws(\" \", col(\"title\"), col(\"selftext\")))\n",
    "    .withColumn(\"timestamp\",\n",
    "                from_unixtime(col(\"created_utc\")).cast(TimestampType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190d8b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic.load(\"modeling/bertopic\")\n",
    "info = topic_model.get_topic_info()  \n",
    "label_map = dict(zip(info[\"Topic\"], info[\"Name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62b63a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are streaming!\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 6e4d6a76-aac1-4e2d-99ef-1d56e4b7a868, runId = d6359008-c770-4477-96ee-05e0e3fa62ce] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\utils.py\", line 120, in call\n    raise e\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\utils.py\", line 117, in call\n    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n  File \"C:\\Users\\tuanq\\AppData\\Local\\Temp\\ipykernel_25832\\2239294911.py\", line 48, in save_batch\n    pdf.to_csv(csv_path, mode=\"a\", index=False, header=(epoch_id==0))\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 333, in wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\", line 3986, in to_csv\n    return DataFrameRenderer(formatter).to_csv(\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\format.py\", line 1014, in to_csv\n    csv_formatter.save()\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\", line 251, in save\n    with get_handle(\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py\", line 873, in get_handle\n    handle = open(\nPermissionError: [Errno 13] Permission denied: 'output/all_results.csv'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 93\u001b[0m\n\u001b[0;32m     84\u001b[0m query \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     85\u001b[0m     cleaned\u001b[38;5;241m.\u001b[39mwriteStream               \u001b[38;5;66;03m# file sink only supports append\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(save_batch)        \u001b[38;5;66;03m# where to write the CSV files\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     90\u001b[0m )\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Chờ query hoàn thành\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\streaming\\query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark-3.5.5-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = 6e4d6a76-aac1-4e2d-99ef-1d56e4b7a868, runId = d6359008-c770-4477-96ee-05e0e3fa62ce] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\utils.py\", line 120, in call\n    raise e\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\utils.py\", line 117, in call\n    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n  File \"C:\\Users\\tuanq\\AppData\\Local\\Temp\\ipykernel_25832\\2239294911.py\", line 48, in save_batch\n    pdf.to_csv(csv_path, mode=\"a\", index=False, header=(epoch_id==0))\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 333, in wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\", line 3986, in to_csv\n    return DataFrameRenderer(formatter).to_csv(\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\format.py\", line 1014, in to_csv\n    csv_formatter.save()\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\", line 251, in save\n    with get_handle(\n  File \"c:\\Users\\tuanq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py\", line 873, in get_handle\n    handle = open(\nPermissionError: [Errno 13] Permission denied: 'output/all_results.csv'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.streaming import StreamingQuery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import (\n",
    "    from_json, col, concat_ws, from_unixtime, pandas_udf\n",
    ")\n",
    "from pyspark.sql.functions import split, regexp_replace, col\n",
    "from pyspark.sql.types import StringType\n",
    "import os\n",
    "\n",
    "MODEL_DIR = \"./modeling/bertopic\"\n",
    "_betopic = None\n",
    "def _get_model():\n",
    "    global _betopic\n",
    "    if _betopic is None:\n",
    "        _betopic = BERTopic.load(MODEL_DIR)\n",
    "    return _betopic\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def predict_label(texts: pd.Series) -> pd.Series:\n",
    "    m = _get_model()\n",
    "    topics, _ = m.transform(texts.tolist())\n",
    "    # clean \"1_top_words_here\" → \"top words here\"\n",
    "    cleaned = []\n",
    "    for t in topics:\n",
    "        if t is None:\n",
    "            cleaned.append(None)\n",
    "        else:\n",
    "            label = label_map.get(int(t), None)\n",
    "            cleaned.append(label.replace(\"_\",\" \"))\n",
    "    return pd.Series(cleaned)\n",
    "\n",
    "annotated = posts.withColumn(\"topic\", predict_label(col(\"text\")))\n",
    "\n",
    "csv_path        = \"output/all_results.csv\"\n",
    "checkpoint_path = \"output/checkpoints/bertopic_fbb\"\n",
    "\n",
    "# fresh start\n",
    "os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "if os.path.exists(csv_path):\n",
    "    os.remove(csv_path)\n",
    "    \n",
    "def save_batch(df, epoch_id):\n",
    "    pdf = df.select(\"text\",\"topic\").toPandas()\n",
    "    if pdf.empty:\n",
    "        return\n",
    "    # append, header only on first batch\n",
    "    pdf.to_csv(csv_path, mode=\"a\", index=False, header=(epoch_id==0))\n",
    "\n",
    "if posts.isStreaming:\n",
    "    print(\"We are streaming!\")\n",
    "    # (posts.writeStream\n",
    "    #     .foreachBatch(foreach_batch)\n",
    "    #     .option(\"checkpointLocation\", \"./tmp/checkpoints/bertopic\")\n",
    "    #     .trigger(processingTime=\"5 seconds\")\n",
    "    #     .start()\n",
    "    #     .awaitTermination())\n",
    "    \n",
    "    # query = (\n",
    "    #     annotated.writeStream\n",
    "    #     # DataStream queries need to be named\n",
    "    #     .queryName(\"posts\")\n",
    "    #     .format(\"memory\")\n",
    "    #     .outputMode(\"append\")\n",
    "    #     .trigger(processingTime=\"5 seconds\")\n",
    "    #     .option(\"checkpointLocation\", r\"C:\\tmp\\spark-checkpoint\\posts_append_v23\")\n",
    "    #     .start()\n",
    "    # )\n",
    "    \n",
    "    # import time\n",
    "    # while query.isActive:\n",
    "    #     # 2) Clear the terminal window\n",
    "    #     os.system('clear')\n",
    "    #     time.sleep(5)  # phải dài hơn trigger interval\n",
    "    #     print(\"=== Latest batch snapshot ===\")\n",
    "    #     spark.table(\"posts\").select(\"text\", \"topic\").show(truncate=False)\n",
    "    \n",
    "    cleaned = annotated \\\n",
    "    .select(\n",
    "        col(\"text\"),\n",
    "        col(\"topic\")\n",
    "    )\n",
    "    \n",
    "    query = (\n",
    "        cleaned.writeStream               # file sink only supports append\n",
    "        .foreachBatch(save_batch)        # where to write the CSV files\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .trigger(processingTime=\"5 seconds\")\n",
    "        .start()\n",
    "    )\n",
    "    \n",
    "    # Chờ query hoàn thành\n",
    "    query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
